{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troika dataset functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    return data[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FourierTransform(signal, fs, factor=6):\n",
    "    \"\"\"\n",
    "    Computes the frequencies and the magnitudes of the Fourier Transform\n",
    "    over a signal\n",
    "    \n",
    "    Args:\n",
    "        signal: (np.array) the signal to be transformed\n",
    "        fs: (number) sampling rate\n",
    "    \n",
    "    Returns:\n",
    "        frequencies and magnitudes of the signal\n",
    "    \"\"\"\n",
    "    window_length = factor * len(signal)\n",
    "    \n",
    "    frequencies = np.fft.rfftfreq(window_length, 1 / fs)\n",
    "    fft = np.abs(np.fft.rfft(signal, window_length))\n",
    "    \n",
    "    return frequencies, fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ButterworthFilter(signal, pass_band=(40/60, 240/60), fs=125):\n",
    "    \"\"\"\n",
    "    Butterworth filter algorithm.\n",
    "    \n",
    "    Returns:\n",
    "        Bandpass filtered signal\n",
    "    \"\"\"          \n",
    "    b, a = sp.signal.butter(2, pass_band, btype='bandpass', fs = fs)\n",
    "    return sp.signal.filtfilt(b, a, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_signal(signal):\n",
    "    signal[signal <= 40/60] = 0\n",
    "    signal[signal >= 240/60] = 0\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Featurize(ppg, accx, accy, accz, fs):\n",
    "    \"\"\"\n",
    "    Featurization of the signal\n",
    "    \n",
    "    Args:\n",
    "        ppg: (np.array) photoplethysmography signal\n",
    "        accx: (np.array) x-channel of the accelerometer signal\n",
    "        accy: (np.array) y-channel of the accelerometer signal\n",
    "        accz: (np.array) z-channel of the accelerometer signal\n",
    "        fs: (number) sampling rate of the accelerometer signal\n",
    "    \n",
    "    Returns:\n",
    "        n-tuple of PPG and ACC features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute accelerator value with its three channels\n",
    "    acc = np.sqrt(np.sum(np.array([accx, accy, accz]) ** 2, axis=0))\n",
    "    \n",
    "    # Fourier Transform\n",
    "    ppg_freqs, ppg_fft = FourierTransform(ppg, fs)\n",
    "    acc_freqs, acc_fft = FourierTransform(acc, fs)\n",
    "\n",
    "    # Filter ffts\n",
    "    ppg_fft = filter_signal(ppg_fft)\n",
    "    acc_fft = filter_signal(acc_fft)\n",
    "    \n",
    "    # Features\n",
    "    ppg_mean_freq = np.mean(ppg_freqs)\n",
    "    ppg_mean_fft = np.mean(ppg_fft)\n",
    "    ppg_max_freq = ppg_freqs[np.argmax(ppg_fft)]\n",
    "    acc_mean_freq = np.mean(acc_freqs)\n",
    "    acc_mean_fft = np.mean(acc_fft)\n",
    "    acc_max_freq = acc_freqs[np.argmax(acc_fft)]\n",
    "    \n",
    "    return (ppg_mean_freq, \n",
    "            ppg_mean_fft, \n",
    "            ppg_max_freq,\n",
    "            acc_mean_freq,\n",
    "            acc_mean_fft,\n",
    "            acc_max_freq,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability. \n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and corresponding \n",
    "            reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse rate\n",
    "            error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, data_fls, ref_fls, fs=100, window_length=8, window_shift=2):\n",
    "        # Data\n",
    "        self.data_fls = data_fls\n",
    "        self.ref_fls = ref_fls\n",
    "        # Hyperparameters\n",
    "        self.fs = fs\n",
    "        self.window_length = window_length\n",
    "        self.window_shift = window_shift\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_fls)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Lists to store features, labels and signals\n",
    "        features, labels, signals = [], [], []\n",
    "        \n",
    "        # Extract data\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(self.data_fls[idx])\n",
    "        bpms = sp.io.loadmat(self.ref_fls[idx])['BPM0'][:, 0]\n",
    "\n",
    "        # Process data\n",
    "        total_windows = min(len(ppg), len(bpms))\n",
    "        left_indexes = np.arange(total_windows, dtype=int) * self.fs * self.window_shift\n",
    "        right_indexes = left_indexes + self.fs * self.window_length\n",
    "        \n",
    "        # Iterate over windows\n",
    "        for i, (left, right) in enumerate(zip(left_indexes, right_indexes)):\n",
    "            # Extract portions\n",
    "            ppg_win = ButterworthFilter(ppg[left:right])\n",
    "            accx_win = ButterworthFilter(accx[left:right])\n",
    "            accy_win = ButterworthFilter(accy[left:right])\n",
    "            accz_win = ButterworthFilter(accz[left:right])\n",
    "            # Save features, labels and signals\n",
    "            features.append(Featurize(ppg_win, accx_win, accy_win, accz_win, self.fs))\n",
    "            labels.append(bpms[i])\n",
    "            signals.append([ppg_win, accx_win, accy_win, accz_win])\n",
    "\n",
    "        return features, labels, signals, (ppg, accx, accy, accz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Top-level class for the estimator model\n",
    "    \n",
    "    Generates a RandomForestRegressor model and provides functions to interact with it.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, processor, n_estimators=100, max_depth=10):\n",
    "        # Parameters\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        # Model\n",
    "        self.model = RandomForestRegressor()\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.processor = processor\n",
    "    \n",
    "    def train(self):\n",
    "        # Generate features\n",
    "        features, labels, _ = self.generate_features()\n",
    "        # Split dataset\n",
    "        X_train, _, y_train, _ = train_test_split(features, labels, test_size=0.2)\n",
    "        # Train model\n",
    "        self.model.fit(X_train, y_train)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "            \n",
    "    def generate_features(self):\n",
    "        features, labels, signals = [], [], []\n",
    "\n",
    "        for i in range(len(self.processor)):\n",
    "            _features, _labels, _signals, _ = self.processor[i]\n",
    "            features.extend(_features)\n",
    "            labels.extend(_labels)\n",
    "            signals.extend(_signals)\n",
    "\n",
    "        return np.array(features), np.array(labels), np.array(signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunPulseRateAlgorithm(data_fls, ref_fls):\n",
    "    \"\"\"\n",
    "    Runs the pulse rate algorithm on the given data files, trains a model and evaluates its\n",
    "    performance on the data computing the MAE and the confidence metric.\n",
    "    \"\"\"\n",
    "    # Generate classes\n",
    "    processor = DataProcessor(data_fls, ref_fls)\n",
    "    model = Estimator(processor=processor)\n",
    "    # Train model\n",
    "    model.train()\n",
    "\n",
    "    # Predict\n",
    "    errors, confidences = [], []\n",
    "    for i in range(len(processor)):\n",
    "        # Predict\n",
    "        features, labels, signals, _ = processor[i]\n",
    "        predictions = model.predict(features)\n",
    "\n",
    "        _errors, _confidences = [], []\n",
    "        for i in range(predictions.shape[0]):\n",
    "            ppg = ButterworthFilter(signals[i][0])\n",
    "\n",
    "            freqs, fft = FourierTransform(ppg, processor.fs, 2)\n",
    "            fft[fft <= 40/60] = 0\n",
    "            fft[fft >= 240/60] = 0\n",
    "\n",
    "            pred_fs = predictions[i] / 55\n",
    "            pred_fs_win = (freqs >= pred_fs - 0.5) & (freqs <= pred_fs + 0.5)\n",
    "            confs = np.sum(fft[pred_fs_win]) / (np.sum(fft) + 1e-6)\n",
    "            _confidences.append(confs)\n",
    "            _errors.append(np.abs(predictions[i] - labels[i]))\n",
    "        \n",
    "        errors.append(_errors)\n",
    "        confidences.append(_confidences)\n",
    "    \n",
    "    errors = np.hstack(errors)\n",
    "    confidences = np.hstack(confidences)\n",
    "    \n",
    "    return errors, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    \n",
    "    # Run the pulse rate algorithm on each trial in the dataset\n",
    "    errors, confidences = RunPulseRateAlgorithm(data_fls, ref_fls)\n",
    "        \n",
    "    return AggregateErrorMetric(errors, confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.043776496326208"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "\n",
    "Answer the following prompts to demonstrate understanding of the algorithm you wrote for this specific context.\n",
    "\n",
    "> - **Code Description** - Include details so someone unfamiliar with your project will know how to run your code and use your algorithm. \n",
    "> - **Data Description** - Describe the dataset that was used to train and test the algorithm. Include its short-comings and what data would be required to build a more complete dataset.\n",
    "> - **Algorithhm Description** will include the following:\n",
    ">   - how the algorithm works\n",
    ">   - the specific aspects of the physiology that it takes advantage of\n",
    ">   - a describtion of the algorithm outputs\n",
    ">   - caveats on algorithm outputs \n",
    ">   - common failure modes\n",
    "> - **Algorithm Performance** - Detail how performance was computed (eg. using cross-validation or train-test split) and what metrics were optimized for. Include error metrics that would be relevant to users of your algorithm. Caveat your performance numbers by acknowledging how generalizable they may or may not be on different datasets.\n",
    "\n",
    "### Answers\n",
    "\n",
    "> - **Code Description** - The structure of the notebook is as follows:\n",
    ">   - *Troika Dataset Functions*: these blocks contain the functions in charge of loading the dataset data.\n",
    ">   - *Auxiliar Functions*: these blocks contain common functions throughout the project.\n",
    ">   - *Classes*: these blocks contain the two main classes that englobe the project. The first one, DataProcessor, is in class in charge of handling the dataset samples and processing them. The second one, Estimator, is the one containing the model and the interfaces to use it for training and predicting.\n",
    ">   - *Main functions*: finally, these blocks contain the top-level functions running the code.\n",
    "> - **Data Description** - The dataset is composed of PPG and accelerator signals as well as BPMs for each of the samples present in the data. \n",
    "> - **Algorithhm Description** will include the following:\n",
    ">   - how the algorithm works: the data is first process into eight second windows in the range of 40 to 240 bpm with a separation of two seconds between each of them. The three channels of the accelerometer are combined to get the magnitude of the accelerometer itself to later be processed into three features along the three more extracted from the PPG.\n",
    ">   - the specific aspects of the physiology that it takes advantage of: the PPG values are obtain by emitting light into the wrists which is absorbed by the red blood cells present in the veins. Every time the veins are fulfill with blood, these causes an increase in the amount of blood cells, absorbing more light. On the other hand, when the blood pressure decreases, less light is absorbed due to the decrease of blood cells' presence. Moreover, the accelerometer signals keep track of the movement of the wrists, giving information about what is going at a physical level to contrast with any rare increase or decrease in the blood pressure apart from the heart beats.\n",
    ">   - a describtion of the algorithm outputs: the algorithm returns the estimated frequency and the confidence of the prediction.\n",
    ">   - caveats on algorithm outputs: the confidence is computed by adding up the magnitudes of the frequencies where its value was close to the model's estimation and dividing them by the total magnitude.\n",
    ">   - common failure modes: any change in the PPG due to a wrist, arm or finger movement.\n",
    "> - **Algorithm Performance** - The main metric used to measure the model's performance was the Mean Absolute Error (MAE) between the heart rate estimated and the expected one. The final layer of the evaluation takes this metrics and computes the MAE at 90% availability, giving a result of `8.044`. It is important to consider that the dataset was composed of only 12 people, which is a low amount of individual samples and the results could be biased to any statistical deviation present in this group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** (back in the Project Classroom) to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
